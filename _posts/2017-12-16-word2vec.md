---
layout: post
title: 对于Word2vec的理解
description:
date: 2017-12-16 21:41:22 +0800
image: assets/images/thumbnail/word2vec.png
---

word2vec也叫word embeddings，中文名“词向量”，作用就是将自然语言中的字词转为计算机可以理解的稠密向量（Dense Vector）。
所以在聊 Word2vec 之前，先讨论NLP(自然语言处理)。NLP里面，最细粒度的是**词语**，词语组成句子，句子再组成段落、篇章、文档。
所以处理 NLP 的问题，首先就要拿词语开刀。
在word2vec出现之前，自然语言处理经常把字词转为离散的单独的符号，也就是One-Hot Encoder。

举个简单例子，判断一个词的词性，是动词还是名词。
用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的词性，我们要构建 f(x)->y 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入。
而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec，就是词嵌入（ word embedding) 的一种。

### 主要模型
Word2Vec模型中，主要有Skip-Gram和CBOW两种模型。
如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做 **Skip-Gram模型**；
而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 **CBOW模型**。
![](http://wx3.sinaimg.cn/large/823422f6ly1fnzkj6uj9oj20k00c43yq.jpg)
