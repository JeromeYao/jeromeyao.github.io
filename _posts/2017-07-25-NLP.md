---
layout: post
title: 中文自然语言处理方法
description:  
date: 2017-07-25 21:41:22 +0800
image: assets/images/thumbnail/nlp.png
---
## WHAT？
自然语言处理，也被称为计算机语言学，结合了机器学习、人工智能和允许我们和类人机器对话的语言学。然语言处理（NLP）与计算机视觉（CV）两者是当前人工智能领域里最为重要的两个发展方向。
互联网刚兴起之时，我们通过使用关键词结合“与、或、非”的布尔搜索术语来实现有效的谷歌搜索。为了得到你想从谷歌得到的答案，你得知道它的语言，用机器能够理解的方式去进行操作。
然后，谷歌引入了语义搜索。它的算法学习了单词之间的关联，使你能够把谷歌当作朋友一样询问问题。在计算机内部，它把这个问题翻译成计算机能理解的布尔结构化搜索——但这些仍然是机器语言。
自然语言处理就是让机器能“理解”人类语言内含的思想并返回想要的结果。
什么是中文自然语言处理？这就不必赘述了。

## WHY？
为什么要做自然语言处理？因为**文本数据无处不在**。
个人认为自然语言处理最大的目标便是将现有的人机交互方式转变为”人际交互”方式，废弃用户图形界面——甚至是用户界面，让与机器互动变得与人对话一样简单。
这样就能降低或彻底移除阻挡人们拥抱大数据的屏障。
回顾历史，互联网的发展极大地降低了人与人之间的沟通成本。
就像木心先生的现代诗《从前慢》中那样描述：
>从前的日色变得慢
>车，马，邮件都慢
>一生只够爱一个人

但是随着我们生活节奏将变得越来越快，个人将会越来越多的数据需要处理，能够理解人类语言的数字助手将会是下一次互联网革命。

在商业领域有几个重要应用出现得尤其频繁：
* 识别不同的用户/客户群（例如预测客户流失、顾客终身价值、产品偏好）
* 准确检测和提取不同类别的反馈（正面和负面的评论/意见，提到的特定属性，如衣服尺寸/合身度等）
* 根据意图对文本进行分类（例如寻求一般帮助，紧急问题）

应用到医疗健康领域，那么**降低就医成本**和**提高就医效率**这两个老大难问题将迎刃而解。

## HOW？
如何来做中文自然语言处理？首先在此梳理一下处理文本语义分析的业务逻辑，在工作中处理业务往往会以结果导向来思考问题，会忽视了方法论的重要性。这一本末倒置的现象大大影响了工作的效率，极端情况需要将整个业务逻辑推倒重来，得不偿失。
我们从以下四个部分来开展讨论如何处理中文自然语言：**文本基本处理**，**文本语义分析**，**图片语义分析**，**语义分析小结**。

首先探讨文本处理的基本方法，这无疑构成了语义分析的基础。接着分文本和图片两节讲述各自语义分析的一些方法，虽然分为两节，但文本和图片在语义分析方法上有很多共通与关联之处。最后我们简单介绍下语义分析的实际应用，并展望一下未来的语义分析方法的前景。

## 文本基本处理
文本基本处理有很多方面，这里主要介绍中文分词以及Term Weighting及Python实现中文自然语言处理的方法。

### 中文分词

通常处理文本工作，拿到一段文本后，首先应该想三个问题：**WHAT？WHY？HOW？**  
这些文本内容是什么？为什么要处理这些文本？怎样处理这些文本？
想清楚这三个问题后，首先应当做的处理工作是分词。分词的方法一般有如下几种：

1. 基于字符串匹配的分词方法。此方法按照不同的扫描方式，逐个查找词库进行分词。根据扫描方式可细分为：正向最大匹配，反向最大匹配，双向最大匹配，最小切分(即最短路径)；总之就是各种不同的启发规则。
2. 全切分方法。它首先切分出与词库匹配的所有可能的词，再运用统计语言模型决定最优的切分结果。它的优点在于可以解决分词中的歧义问题。下图是一个示例，对于文本串“南京市长江大桥”，首先进行词条检索(一般用Trie存储)，找到匹配的所有词条（南京，市，长江，大桥，南京市，长江大桥，市长，江大桥，江大，桥），以词网格(word lattices)形式表示，接着做路径搜索，基于统计语言模型(例如[n-gram](https://en.wikipedia.org/wiki/N-gram))找到最优路径，最后可能还需要命名实体识别。下图中“南京市 长江 大桥”的语言模型得分，即P(南京市，长江，大桥)最高，则为最优切分。
![](http://wx4.sinaimg.cn/large/823422f6ly1fnssnznm1sj208r02djr8.jpg)  
3. 由字构词的分词方法。可以理解为字的分类问题，也就是自然语言处理中的sequence labeling问题，通常做法里利用HMM，MAXENT，MEMM，CRF等预测文本串每个字的tag，譬如B，E，I，S，这四个tag分别表示：beginning, inside, ending, single，也就是一个词的开始，中间，结束，以及单个字的词。 例如“南京市长江大桥”的标注结果可能为：“南(B)京(I)市(E)长(B)江(E)大(B)桥(E)”。由于CRF既可以像最大熵模型一样加各种领域feature，又避免了HMM的齐次马尔科夫假设，所以基于CRF的分词目前是效果最好的。除了HMM，CRF等模型，分词也可以基于深度学习方法来做，也取得了state-of-the-art的结果。

![](http://wx1.sinaimg.cn/large/823422f6ly1fnsswzv7z1j209z0e1jt0.jpg)

上图是一个基于深度学习的分词示例图。我们从上往下看，首先对每一个字进行Lookup Table，映射到一个固定长度的特征向量(这里可以利用词向量，boundary entropy，accessor variety等)；接着经过一个标准的神经网络，分别是linear，sigmoid，linear层，对于每个字，预测该字属于B,E,I,S的概率；最后输出是一个矩阵，矩阵的行是B,E,I,S 4个tag，利用viterbi算法就可以完成标注推断，从而得到分词结果。
一个文本串除了分词，还需要做词性标注，命名实体识别，新词发现等。通常有两种方案，一种是pipeline approaches，就是先分词，再做词性标注；另一种是joint approaches，就是把这些任务用一个模型来完成。
一般而言，方法一和方法二在工业界用得比较多，方法三因为采用复杂的模型，虽准确率相对高，但耗时较大。

### 语言模型
前面在讲“全切分分词”方法时，提到了语言模型，并且通过语言模型，还可以引出词向量，所以这里把语言模型简单阐述一下。

语言模型是用来计算一个句子产生概率的概率模型，即`P(w_1,w_2,w_3…w_m)`，m表示词的总个数。根据贝叶斯公式：`P(w_1,w_2,w_3 … w_m) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2) … P(w_m|w_1,w_2 … w_{m-1})`。

最简单的语言模型是N-Gram，它利用马尔科夫假设，认为句子中每个单词只与其前n–1个单词有关，即假设产生w_m这个词的条件概率只依赖于前n–1个词，则有`P(w_m|w_1,w_2…w_{m-1}) = P(w_m|w_{m-n+1},w_{m-n+2} … w_{m-1})`。其中n越大，模型可区别性越强，n越小，模型可靠性越高。

N-Gram语言模型简单有效，但是它只考虑了词的位置关系，没有考虑词之间的相似度，词语法和词语义，并且还存在数据稀疏的问题，所以后来，又逐渐提出更多的语言模型，例如Class-based ngram model，topic-based ngram model，cache-based ngram model，skipping ngram model，指数语言模型（最大熵模型，条件随机域模型）等。

最近，随着深度学习的兴起，神经网络语言模型也变得火热。用神经网络训练语言模型的经典之作，要数Bengio等人发表的《A Neural Probabilistic Language Model》[3]，它也是基于N-Gram的，首先将每个单词w_{m-n+1},w_{m-n+2} … w_{m-1}映射到词向量空间，再把各个单词的词向量组合成一个更大的向量作为神经网络输入，输出是P(w_m)。本文将此模型简称为ffnnlm（Feed-forward Neural Net Language Model）。ffnnlm解决了传统n-gram的两个缺陷：(1)词语之间的相似性可以通过词向量来体现；(2)自带平滑功能。不仅提出神经网络语言模型，还顺带引出了词向量。
![](http://wx4.sinaimg.cn/large/823422f6ly1fnstb7oi0bj20dc0akab7.jpg)
从最新文献看，目前state-of-the-art语言模型应该是基于循环神经网络(recurrent neural network)的语言模型，简称rnnlm。循环神经网络相比于传统前馈神经网络，其特点是：可以存在有向环，将上一次的输出作为本次的输入。而rnnlm和ffnnlm的最大区别是：ffnnmm要求输入的上下文是固定长度的，也就是说n-gram中的 n 要求是个固定值，而rnnlm不限制上下文的长度，可以真正充分地利用所有上文信息来预测下一个词，本次预测的中间隐层信息(例如下图中的context信息)可以在下一次预测里循环使用。
![](http://wx4.sinaimg.cn/large/823422f6ly1fnstcu8g3kj2088093glv.jpg)
如上图所示，这是一个最简单的rnnlm，神经网络分为三层，第一层是输入层，第二层是隐藏层(也叫context层)，第三层输出层。 假设当前是t时刻，则分三步来预测P(w_m)：

* 单词w_{m-1}映射到词向量，记作input(t)
* 连接上一次训练的隐藏层context(t–1)，经过sigmoid function，生成当前t时刻的context(t)
* 利用softmax function，预测P(w_m)
参考文献[7]中列出了一个rnnlm的library，其代码紧凑。利用它训练中文语言模型将很简单，上面“南京市 长江 大桥”就是rnnlm的预测结果。

基于RNN的language model利用BPTT(BackPropagation through time)算法比较难于训练，原因就是深度神经网络里比较普遍的vanishing gradient问题[55]（在RNN里，梯度计算随时间成指数倍增长或衰减，称之为Exponential Error Decay）。所以后来又提出基于LSTM(Long short term memory)的language model，LSTM也是一种RNN网络，关于LSTM的详细介绍请参考文献[54,49,52]。LSTM通过网络结构的修改，从而避免vanishing gradient问题。

![](http://wx1.sinaimg.cn/large/823422f6ly1fnstyny1dpj20ag08tglw.jpg)
如上图所示，是一个LSTM unit。如果是传统的神经网络unit，output activation bi = activation_function(ai)，但LSTM unit的计算相对就复杂些了，它保存了该神经元上一次计算的结果，通过input gate，output gate，forget gate来计算输出。

### Term Weighting
Term重要性
对文本分词后，接下来需要对分词后的每个term计算一个权重，重要的term应该给与更高的权重。举例来说，“什么产品对减肥帮助最大？”的term weighting结果可能是: “什么 0.1，产品 0.5，对 0.1，减肥 0.8，帮助 0.3，最大 0.2”。Term weighting在文本检索，文本相关性，核心词提取等任务中都有重要作用。

* Term weighting的打分公式一般由三部分组成：local，global和normalization [1,2]。即TermWeight=L_{i,j} G_i N_j。L_{i,j}是term i在document j中的local weight，G_i是term i的global weight，N_j是document j的归一化因子。

常见的local，global，normalization weight公式:

![](http://wx2.sinaimg.cn/large/823422f6ly1fnszly3io0j20bj097t9a.jpg)
![](http://wx1.sinaimg.cn/large/823422f6ly1fnszm50kncj209x083mxp.jpg)
![](http://wx3.sinaimg.cn/large/823422f6ly1fnszm7d49vj20ak04sdg5.jpg)

Tf-Idf是一种最常见的term weighting方法。在上面的公式体系里，Tf-Idf的local weight是FREQ，glocal weight是IDFB，normalization是None。tf是词频，表示这个词出现的次数。df是文档频率，表示这个词在多少个文档中出现。idf则是逆文档频率，idf=log(TD/df)，TD表示总文档数。Tf-Idf在很多场合都很有效，但缺点也比较明显，以“词频”度量重要性，不够全面，譬如在搜索广告的关键词匹配时就不够用。

除了TF-IDF外，还有很多其他term weighting方法，例如Okapi，MI，LTU，ATC，TF-ICF[59]等。通过local，global，normalization各种公式的组合，可以生成不同的term weighting计算方法。不过上面这些方法都是无监督计算方法，有一定程度的通用性，但在一些特定场景里显得不够灵活，不够准确，所以可以基于有监督机器学习方法来拟合term weighting结果。

![](http://wx4.sinaimg.cn/large/823422f6ly1fnszmag8vtj20d203wq37.jpg)

利用有监督机器学习方法来预测weight。这里类似于机器学习的分类任务，对于文本串的每个term，预测一个[0,1]的得分，得分越大则term重要性越高。既然是有监督学习，那么就需要训练数据。如果采用人工标注的话，极大耗费人力，所以可以采用训练数据自提取的方法，利用程序从搜索日志里自动挖掘。从海量日志数据里提取隐含的用户对于term重要性的标注，得到的训练数据将综合亿级用户的“标注结果”，覆盖面更广，且来自于真实搜索数据，训练结果与标注的目标集分布接近，训练数据更精确。下面列举三种方法(除此外，还有更多可以利用的方法)：
* 从搜索session数据里提取训练数据，用户在一个检索会话中的检索核心意图是不变的，提取出核心意图所对应的term，其重要性就高。
* 从历史短串关系资源库里提取训练数据，短串扩展关系中，一个term出现的次数越多，则越重要。
* 从搜索广告点击日志里提取训练数据，query与bidword共有term的点击率越高，它在query中的重要程度就越高。
通过上面的方法，可以提取到大量质量不错的训练数据（数十亿级别的数据，这其中可能有部分样本不准确，但在如此大规模数据情况下，绝大部分样本都是准确的）。

有了训练数据，接下来提取特征，基于逻辑回归模型来预测文本串中每个term的重要性。所提取的特征包括：

* term的自解释特征，例如term专名类型，term词性，term idf，位置特征，term的长度等；
* term与文本串的交叉特征，例如term与文本串中其他term的字面交叉特征，term转移到文本串中其他term的转移概率特征，term的文本分类、topic与文本串的文本分类、topic的交叉特征等。

### 核心词、关键词提取
短文本串的核心词提取。对短文本串分词后，利用上面介绍的term weighting方法，获取term weight后，取一定的阈值，就可以提取出短文本串的核心词。
长文本串(譬如web page)的关键词提取。这里简单介绍几种方法。想了解更多，请参考文献[69]。
采用基于规则的方法。考虑到位置特征，网页特征等。
基于广告主购买的bidword和高频query建立多模式匹配树，在长文本串中进行全字匹配找出候选关键词，再结合关键词weight，以及某些规则找出优质的关键词。
类似于有监督的term weighting方法，也可以训练关键词weighting的模型。
基于文档主题结构的关键词抽取，具体可以参考文献[71]。











## Python实现语义分析
使用Python处理中文分词有这几个库，jieba分词、NLKT、THULAC:

1. [fxsjy/jieba](https://github.com/fxsjy/jieba/) SLOGEN：做最好的Python中文分词组件，或许从现在来看它还没做到最好，但是已经做到了使用的人最多。结巴分词网上的学习资料和使用案例比较多，上手相对比较轻松，速度也比较快。之前微信好友签名词云的小案例中就使用到了jieba分词来提取签名中的关键字。
结巴的优点：
* 支持三种分词模式  
* 支持繁体分词  
* 支持自定义词典  
* MIT 授权协议  

2. THULAC：一个高效的中文词法分析工具包前两天我在做有关于共享单车的用户反馈分类，使用jieba分词一直太过零散，分类分不好。THULAC： 由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包 。THULAC的接口文档很详细，简单易上手。
THULAC分词的优点：
* 能力强。利用规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。
* 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％。
* 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度达到1.3MB/s，速度比jieba慢。  

3. NLTK：[Natural Language Toolkit](http://www.nltk.org/)可以说是世界范围最主流的Python自然语言处理库。主要是处理英文对象，相比于前两个库对中文支持就没那么好了。

4. [Synonyms](https://github.com/huyingxi/Synonyms): 中文近义词工具包


http://www.52nlp.cn/synonyms-%E4%B8%AD%E6%96%87%E8%BF%91%E4%B9%89%E8%AF%8D%E5%B7%A5%E5%85%B7%E5%8C%85
