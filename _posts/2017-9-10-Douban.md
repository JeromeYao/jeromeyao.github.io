---
layout: post
title: 浅析豆瓣电影TOP250榜单
description: 
date: 2017-09-10 14:43:15 +0800
image: assets/images/filmCloud.png
---

![filmcloud](/assets/images/filmCloud.png)

> 豆瓣用户每天都在对“看过”的电影进行“很差”到“力荐”的评价，豆瓣根据每部影片看过的人数以及该影片所得的评价等综合数据，通过算法分析产生豆瓣电影250。

豆瓣网创始人兼CEO杨勃在知乎的[豆瓣电影的分数和排序是怎么算出来的？](https://www.zhihu.com/question/19627832)一问中的回答如下：  
> 豆瓣250里排序是综合分数和人数产生的，这个和IMDB总的想法类似。每一部电影的分数，确实主要是平均分数，但不简简单单是。因为偶然要和影托或者其他非正常个人意见PK，算法考虑了很多因素，包括时间和打分者自身的情况。细节不便公开，而且经常在细调。原则是尽算法范畴的所有能力去接近和还原普通观众最原汁原味的平均观影意见。有一个因素从来没有考虑过，就是商业合作。只要我在豆瓣，商业合作和分数不会有任何关系。  


作为一篇数据分析的文章，本文来探究一下其中所谓的算法的奥秘。

## 概要  

[豆瓣网](https://www.douban.com)是一个国内用户分享交流电影、书籍等文化活动的社交平台。该网站以书影音起家，提供关于书籍、电影、音乐等作品的信息，无论描述还是评论都由用户提供（User-generated content，UGC），是Web 2.0网站中具有特色的一个网站。  

豆瓣电影评分的定位相当于中国国内的`IMDb`（互联网电影数据库），而其针对了国内用户区别于国外的文化差异、人群受众、网络交流环境等各方面因素在电影评价上产生了一定的差异，能更贴切地反映国人对于电影的理解。  
本文为[豆瓣电影TOP250](https://movie.douban.com/top250)榜单的分析以及展示数据背后的意义。  

### 目标网页地址  

[https://movie.douban.com/top250](https://movie.douban.com/top250)

### 项目流程如下：  

+ 数据收集 —— 运用`Python`的`requests`, `lxml`等库抓取豆瓣网页数据，并导出为`csv`文件作为数据库。  
+ 数据处理 —— 使用`Python`的`Numpy`, `Pandas`库整理数据。  
+ 分析与展示 —— 根据处理后的数据做分析，并使用`matplotlib`和`sklearn`库做相应的可视化展示与回归分析，使得文章更直观反映数据背后的意义。  
+ 总结 —— 作出结论。  

其中 **数据收集、处理** 这两部分的文章内容主要侧重于技术实现， **分析与展示** 这部分的内容则相对侧重可视化表现以及数据背后的意义。


# 数据收集

我们在抓取信息之前先看一下豆瓣网的[robots协议](https://www.douban.com/robots.txt):

```
User-agent: *
Disallow: /subject_search
Disallow: /amazon_search
Disallow: /search
Disallow: /group/search
Disallow: /event/search
Disallow: /celebrities/search
Disallow: /location/drama/search
Disallow: /forum/
Disallow: /new_subject
Disallow: /service/iframe
Disallow: /j/
Disallow: /link2/
Disallow: /recommend/
Disallow: /trailer/
Disallow: /doubanapp/card
Sitemap: https://www.douban.com/sitemap_index.xml
Sitemap: https://www.douban.com/sitemap_updated_index.xml
# Crawl-delay: 5

User-agent: Wandoujia Spider
Disallow: /
```

可以看到我们要抓取的`/top250`并不在禁止之列，那么在不影响服务器性能的前提下，可以合理的运用爬虫来抓取所需的信息。

抓取信息的第一步，引入`Python`的`HTTP`库`requests`用来模拟浏览器登录网页，解析网页`Html`文档的库`lxml`以及用来匹配文本信息的正则表达式库`re`。

> 由于网页结构相对比较简单，所以这里直接使用`xpath`来定位标签，获取对应所需的信息。其实也可以引入`BeautifulSoup`库简化定位标签节点的过程。

```python
import requests
from lxml import html
import re
```
定义一个抓取函数，其中用到`requests`库的`get`方法模拟`http`的`get`请求来获取信息，得到一个名为`r`的`requests`对象。

```python
def get_html_text(url， headers):
    try:
        r = requests.get(url=url, headers=headers)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text  # 响应内容
    except:
        return 'Gather Error'
```
其中：

1. `raise_for_status()`方法的作用是：若`requests`对象的状态码不为`200`，则引发`HTTPError`异常。
2. `r.encoding`为`HTTP header`中猜测的响应编码方式，`r.apparent_encoding`为从内容中分析出的响应内容编码方式。


根据观察可以看出`250`条电影信息存放在`10`个页面内，使用变量`i`计数,在`0～10`个页面内抓取信息。此函数需要使用变量计数，记录抓取电影的个数，此变量设置为`x`，每个循环内的`x`即为当前页面内抓取的信息条数。抓取页面信息使用的是`requests`库的`get`方法，再使用`text`方法得到页面文本内容。


![Screenshot_douban](/assets/Screenshot_douban_source.png)

观察网页源码可以看出，所有的信息都在每个`class`属性为`info`的`div`标签里。依此类推定位到各信息所在标签，代码如下：

```python
def douban_top250_spyder(text, x):  # 用于定位信息
    # 所有的信息都在class属性为info的div标签里
    for j in text.xpath('//div[@class="info"]'):
        title = j.xpath('div[@class="hd"]/a/span[@class="title"]/text()')[0]  # 影片名称
        info = j.xpath('div[@class="bd"]/p[1]/text()')  # 信息段
        rate = 9j.xpath('div[@class="bd"]/div[@class="star"]/span[@class="rating_num"]/text()')[0]  # 评分
        com_count0 = j.xpath('div[@class="bd"]/div[@class="star"]/span[4]/text()')[0]  # 评论人数
        com_count = re.match(r'^\d*', com_count0).group()  # 仅保留数字
        quote0 = j.xpath('div[@class="bd"]/p[@class="quote"]/span[@class="inq"]/text()')  # 短评
        quote = '无' if quote0 == [] else quote0[0].replace(",", "，")  # 若短评不存在则使用‘无’替代，并将短评中的英文逗号替换为中文逗号，避免影响CSV文件的处理
        date = info[1].replace("\n", "").strip(' ').split("\xa0/\xa0")[0]  # 上映日期
        country = info[1].split("\xa0/\xa0")[1]  # 制片国家
        genre = info[1].replace("\n", "").strip(' ').split("\xa0/\xa0")[2]  # 影片类型

```

打印出得到的信息，在控制台核查：

```python
print("x" % str(k), title, rate, com_count, date, country, genre, quote)  # 打印结果
```

```
loop 1
1 肖申克的救赎 9.6 835810 1994 美国 犯罪 剧情 希望让人自由。
2 这个杀手不太冷 9.4 801886 1994 法国 剧情 动作 犯罪 怪蜀黍和小萝莉不得不说的故事。
霸王别姬 9.5 597808 1993 中国大陆 香港 剧情 爱情 同性 风华绝代。
4 阿甘正传 9.4 686379 1994 美国 剧情 爱情 一部美国近现代史。
5 美丽人生 9.5 399229 1997 意大利 剧情 喜剧 爱情 战争 最美的谎言。
 ...
loop 10
 ...
23 彗星来的那一夜 8.3 149338 2013 美国 英国 科幻 悬疑 惊悚 小成本大魅力。
24 黑鹰坠落 8.5 101144 2001 美国 动作 历史 战争 还原真实而残酷的战争。
25 假如爱有天意 8.2 216192 2003 韩国 剧情 爱情 琼瑶阿姨在韩国的深刻版。
```

写入所得到的信息，以逗号分割，存为`csv`文件。

```python
with open("douban_top250_demo.csv", "a") as f:  # 写入文件
    f.write("%s,%s,%s,%s,%s,%s,%s\n" % (title, rate, com_count, date, country, genre, quote))
x += 1  # 每条电影信息打印完后计数加一
```

最后，执行代码主体：
```python
headers_douban = {
        'Accept': '*/*',
        'Accept-Encoding': 'gzip, deflate, sdch, br',
        'Accept-Language': 'zh-CN,zh;q=0.8',
        'Connection': 'keep-alive',
        'Referer': 'http://www.douban.com/',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\
         Chrome/58.0.3029.110 Safari/537.36'
    }  # 请求头部

if __name__ == '__main__':  # 执行代码
    for i in range(10):  # 每页25个电影，共10页，程序在其中做循环，抓取信息。
        print('loop', i+1)  # 显示第几圈
        url_douban = 'https://movie.douban.com/top250?start={}&filter='.format(i * 25)  # 目标网站迭代形式
        text0 = get_html_text(url_douban, headers_douban)  # 请求得到的网页文本内容
        text_douban = html.fromstring(text0)  # 转换为html类数据，便于xpath处理获取信息
        num_counting = 1  # 计数
        douban_top250_spyder(text_douban, num_counting)
```

得到的效果如下:

![douban_top250_text](/assets/images/Screenshot_douban_top250_csv.png)


# 数据处理

核对数据收集阶段保存的`douban_top250_demo.csv`文件，确认与预期效果一致后，保存为`douban_top250.csv`用于数据处理。
这样可以避免在数据处理阶段反复向豆瓣服务器发出数据请求，被反爬虫机制屏蔽。

数据处理、分析和展示阶段，我们主要任务是格式化数据，根据处理过的数据来制作相应的分析图像。需要导入以下几个`python`库：

```python
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
from PIL import Image
from collections import Counter
from wordcloud import WordCloud, ImageColorGenerator
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn import linear_model
```

开始处理数据，大致操作思路如下：
先打开文件，再读取整个文件，以逗号分割为列表对象，然后转化为250×7的`ndarray`对象，对应7列数据，每列250个元素，最后将其转换为`DataFrame`对象并加上每列的列名，以方便后续调用。


```python
with open('douban_top250.csv') as f:  # 打开文件
    text_f = f.read().strip()  # 读取文件得到str型文本
list_f = re.split('[,\n]', text_f)  # 分割str文本并转换为list型
array_f = np.array(list_f).reshape(250, 7)  # 转换为250×7的数组
columns_df_f = ['电影名称', '评分', '评分人数', '上映年份', '国家', '类型', '短评']  # 列名
df_f = pd.DataFrame(array_f, columns=columns_df_f)  # 转为DataFrame类
print(df_f)
```

打印得到如下结果：

> ```
  电影名称   评分    评分人数  上映年份   国家    类型  \
0 肖申克的救赎  9.6  833069  1994    美国       犯罪 剧情   
1 这个杀手不太冷  9.4  799185  1994 法国      剧情 动作 犯罪   
2 霸王别姬  9.5  595660  1993   中国大陆 香港   剧情 爱情 同性   
3 阿甘正传  9.4  684515  1994  美国   剧情 爱情   
4 美丽人生  9.5  397936  1997  意大利 剧情 喜剧 爱情 战争   
...      ...      ...   ...    ...   ...    ...
247 彗星来的那一夜  8.3  148590  2013   美国 英国 科幻 悬疑 惊悚   
248 黑鹰坠落  8.5  100794  2001  美国    动作 历史 战争   
249 假如爱有天意  8.2  215610  2003  韩国      剧情 爱情   
               短评  
0             希望让人自由。  
1         怪蜀黍和小萝莉不得不说的故事。  
...              ...
248       还原真实而残酷的战争。  
249          琼瑶阿姨在韩国的深刻版。  
[250 rows x 7 columns]
```

### 片名汇总

`DataFrame`类型的数据在提取列信息方面十分方便。
汇总榜单上所有电影的名字只需将`df_f`对象的`'电影名称'`元素赋给变量即可。

```python
titles = df_f['电影名称']  # 提取片名列  
print(titles)
```

打印结果如下：

> ```
0         肖申克的救赎
1        这个杀手不太冷
2           霸王别姬
3           阿甘正传
4           美丽人生
5           千与千寻
     ...
244         廊桥遗梦
245         罪恶之城
246         两小无猜
247      彗星来的那一夜
248         黑鹰坠落
249       假如爱有天意
Name: 电影名称, Length: 250, dtype: object
```

### 制片国家及影片类型信息处理

处理国家名及影片类型信息的方法与处理片名的方法大同小异。

```python
countries = df_f['国家']  # 提取数组中的国家名数据  
print(countries)
```

这里需要注意观察返回的结果中的：

> ```
2         中国大陆 香港
```

可以看出存在一部影片有多个制作公司的情况，如果要计数则需要解压元素。

```python
list_countries = ' '.join(countries).split()  # 由于存在一部电影有多个制片国家，需要解压出。
print(list_countries)
```
得到列表类的结果如下：  
> ['美国', '法国', '中国大陆', '香港', '美国', '意大利', ... ,'美国', '英国', '美国', '韩国']

对列表元素进行计数,使用内置的`collections`库的`Counter`将列表转化为字典，其中`key`为原列表中的元素，`value`为该元素在列表中出现的次数。    
```python
dict_countries0 = Counter(list_countries)  # 转换成字典类并计数
print(dict_countries0)
```
> Counter({'美国': 143, '英国': 34, '日本': 29, '法国': 27,...,'博茨瓦纳': 1, '爱尔兰': 1})

这样也可以很容易得到总共有多少个不同国家出现：
```python
total_countries = len(dict_countries0)  # 字典长度，即出现的不同国家个数
print(total_countries)
```
> 31

```python
num_countries_show = 12  # 控制参数，显示国家的个数，其余用“其他”来概括
dict_countries = list(dict_countries0.most_common(num_countries_show))  # 提取前12个
countries_rest = dict_countries0.most_common()[num_countries_show:]  # 第12个以后合并为一项
size = [i[1] for i in dict_countries]  #
size.append(sum(i[1] for i in countries_rest))
labels = [i[0] for i in dict_countries]
labels.append('其他')
```

榜单上影片的类型的数据处理与制片国家数据处理类似。  

```python
genres = df_f['类型']
list_genres = ' '.join(genres).split()
dict_genres0 = Counter(list_genres)
total_genres = len(dict_genres0)
num_genres_show = 14  # 控制参数，显示类型的个数，其余用“其他”来概括
dict_genres = list(dict_genres0.most_common(num_genres_show))
genres_rest = dict_genres0.most_common()[num_genres_show:]
size = [i[1] for i in dict_genres]
size.append(sum(i[1] for i in genres_rest))
labels = [i[0] for i in dict_genres]
labels.append('其他')
```


### 上榜年份信息处理  

在年份分析部分除了我们需要注意上榜年份为零的数据是不显示的，需要我们另外添加。  

```python
date_movie = df_f['上映年份']
min_date = int(min(date_movie))
max_date = int(max(date_movie))
range_date = range(min_date, max_date + 1)  # 上榜电影年份范围
dict_date0 = Counter(date_movie)  # 转换为年份与对应出现次数的字典
k = [int(key) for key in dict_date0.keys()]  # 构造上榜年份列表
list_date0 = {key: dict_date0[str(key)] if key in k else 0 for key in range_date}  # 以零填充没有上榜的年份的值
dict_date = Counter(list_date0)  # 转换为字典并计数
keys_date, values_date = list(list_date.keys()), list(list_date.values())
```

### 评分、评论人数处理

抓取得到的评分、评论人数的数据可直接调用，处理方法相对比较简单：  

```python
rate_movie = df_f['评分']
index_rate = rate_movie.index + 1  # 序号即排名（从1开始）
values_rate = rate_movie.values  # 得到评分值列表

comments_count = df_f['评分人数']
values_comments = comments_count.values
```

# 分析与展示

在分析豆瓣榜单排名之前，让我们来考虑一下几个常见的问题：

* 首先从电影方面，一部一百人评价的电影得到的总评分为`9.9`，一部十万人评价的电影得到的总分为`9.0`，这两部影片到底哪一部作品更优秀，应该排在前面？

* 随着互联网的爆炸式发展，新上映的影片受关注度往往会比过往的影片多得多。  
  这种情况最典型的例子就是随着电影业的发展，以往被奉为经典的电影可能还没有最新上映的普通影片在一个月能的评价多。举一个例子，我们对比一下最近上映了一个多月的[《摔跤吧！爸爸》](https://movie.douban.com/subject/26387939/)与四十多年的经典之作[《教父》](https://movie.douban.com/subject/1291841/)的评分：  
  ![gfd](/assets/images/gfd.jpg)  
  可以看到两者的均分和打分人数十分接近，那么这两部影片是不是同样好的呢？  
  这就不得而知了。

* 我们再从豆瓣的平台属性来看，作为综合性社区影片打分的人群并是不一定为了体现电影的优劣。比如某些明星的粉丝或是某些喜欢特定类型片的观众会带有明显的倾向性，例如:  
  ![comments\_lss](/assets/images/lss.png)

以上几点都是不容小觑的问题，会极大的影响榜单的合理性。因此排名算法如何消除这些干扰因素，对于这个榜单来说是一个关键所在。

---

由于豆瓣电影排名的具体算法并未对外公开，只有在[《算法工程师如何改进豆瓣电影 TOP250》](https://blog.douban.com/douban/2013/07/04/2630/)这篇豆瓣日志中谈到了其算法理念，所以我们无法直接从豆瓣网排名算法的角度上讨论。但是文中提到了业界老大哥`IMDb`的榜单算法，豆瓣电影评分的榜单算法或多或少有其影子存在。  
`IMDb`网站是目前互联网上最为权威、系统、全面的电影资料网站，里面包括了几乎所有的电影，以及`1982`年以后的电视剧集。`IMDb`的资料中包括了影片的众多信息，演员，片长，内容介绍，分级 ，评论等，它所特有的电影评分系统深受影迷的欢迎，注册的用户可以给任何一部影片打分并加以评述，而网站又会根据影片所得平均分、选票的数目等计算得出影片的加权平均分并以此进行`TOP250`的排名。  
`IMDb`的算法在其[官方网站](http://www.imdb.com/help/show_leaf?votestopfaq&pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=2398042102&pf_rd_r=1R7MKTKRHQ8ZMG6KXPZZ&pf_rd_s=center-1&pf_rd_t=15506&pf_rd_i=top&ref_=chttp_faq)有如下的解释：

> The following formula is used to calculate the Top Rated 250 titles. This formula provides a true 'Bayesian estimate', which takes into account the number of votes each title has received, minimum votes required to be on the list, and the mean vote for all titles:
>
> weighted rating \(WR\) = \(v ÷ \(v+m\)\) × R + \(m ÷ \(v+m\)\) × C   
> Where:   
> R = average for the movie \(mean\) = \(Rating\)  
> v = number of votes for the movie = \(votes\)  
> m = minimum votes required to be listed in the Top 250  
> C = the mean vote across the whole report

* 第一个参数`R`是该电影按照常规方法算出的评分算术平均数；
* 第二个参数`v`是评分的有效评分人数,需要注意的是这个人数只有符合一定投票要求的人才会被计算在内；
* 第三个参数`m`是进入`IMDb TOP250`榜单需要的最小评分人数；
* 第四个参数`C`是目前所有影片的平均分数。                         

这个方法的本质是一个基础的贝叶斯平均，这个公式的目的为的是通过每部影片的【评分人数】作为调节排序的主要手段：如果这部影片的评分人数低于一个预设值，则影片的最终得分会向全部影片的平均分靠近。然后，随着豆瓣电影排名访问人数的日益增长，`IMDb`将自己的入榜最小值`m`不断调整以适应投票环境的变化。使用动态公式带来的算法优势是可以避免小众电影的排名偏高，很好地解决了我们在本段落提出的问题一。

但是，仅仅依靠贝叶斯平均的方法对于问题二来说是无能为力的。我们需要增加一个时间维度的考察标准来评定，如果该影片仅在上映之初有不错的评价和关注度，之后便持续下滑的话，那么这部影片很难被称为一部好的作品。  

再者，如何有效降低恶意刷分对于排名的影响？这也是大多数公开的算法中没有提到的部分,大抵只能看到些“我们的榜单是不会受到水军影响”之类的豪言壮语。个人认为对于处理此类方法最简单可行的方案便是暂不将热映新片排入TOP250，当影片下映以后脱离商业因素的影响再纳入评判此片是否是一部值得上榜的影片范畴。

---

简单的分析了`IMDb`的排名算法之后，我们可以发现有几个参数对于电影排名起到很重要的作用：**影片的排名**、**电影评分**、**电影评价人数**、**影片上映时间** 等。以下从豆瓣电影`TOP250`榜单获取的数据中分析这几个重要的影响因素及其中关系。

### 片名词云

我们可以将这250个电影名称组成一张电影放映机的形象来当作本文的封面照。

```python
text_titles = r' '.join(titles)  # 列表转换成str类
alice_coloring = np.array(Image.open("film.jpg"))  # 读取mask/color图片
image_colors = ImageColorGenerator(alice_coloring)  # 颜色生成器
my_wordcloud = WordCloud(background_color="white",  # 设置背景颜色
                         max_words=2000,  # 设置最大实现的字数
                         mask=alice_coloring,  # 设置背景照片
                         max_font_size=30,  # 设置字体最大值
                         random_state=1  # 设置随机配色数
                         ).generate(text_titles)  # 根据titles生成词云
plt.imshow(my_wordcloud.recolor(color_func=image_colors))  # 再上色并显示
plt.title(s='Douban Top-250 rated movies analysis', fontsize=20)  # 标题
plt.text(800, 700, 'By: Jerome Yao', fontsize=8)  # 签名
plt.axis("off")  # 不显示x, y轴
plt.savefig('filmCloud_demo.png', dpi=500)  # 保存图片
plt.show()  # 展示图片
```

效果如下：

![filmcloud](/assets/images/filmCloud.png)

### 制片国分析

我们先来简单的看一下上榜影片的制片国家一共有哪些？每个国家上榜了多少次  
环形图来展现各制片国家所占份额可以更清晰地体现各个国家所占的比例。  
当数据条目较多，且需要明显体现各部分的比例时，环形图是个很好的选择。  
由于存在一部影片由多个国家的团队合作完成，上榜电影的制作国家总和是大于`250`的，在此环形图内的数据单位为“次”而非百分比。

```python
font_initial = matplotlib.rcParams['font.family']  # 初始字体
matplotlib.rcParams['font.family'] = 'Droid Sans Fallback'  # 修改字体
ax0 = plt.subplot()  # 创建子作图区
ax0.set_title('各制片国上榜次数统计环形图')  # 标题
ax0.pie(size, labels=labels, shadow=False, startangle=90)  # 做饼图
matplotlib.rcParams['font.family'] = font_initial  # 还原字体
ax0.pie(size, labels=size, radius=0.6, startangle=90, labeldistance=1.2)  # 环内数值
ax0.pie([1], radius=0.6, colors='w')  # 白心
ax0.axis('equal')  # 正圆
plt.savefig('countries_demo.png', dpi=300)  # 保存图片
plt.show()
```

得到如下图像：  
![countries](/assets/images/countries.png)  
其中美国参与制作的影片数量一骑绝尘，如果再加上英国的份额，这两大英语系国家便可以占到所有上榜电影的半壁江山。
再将数据投射到世界地图上来看看各国家或地区的地理位置和上榜次数的关系。
![location](/assets/images/location.png)  
从地理位置角度分析，我们可以明显发现颜色较深即上榜次数较多的地理位置主要分布在欧美和东亚。这一现象体现中国电影文化主要受到两方面因素影响，欧美发达国家的强势文化输入以及自身和就近的东亚文化沉淀。

### 影片类型分析

类型的分布与国家分布类似,同样以环形图来展现。  
这里就不贴代码赘述了，得到图像如下：

![generes](/assets/images/genres.png)

如图，我们可以发现剧情片占绝对主导地位，那些传播“正能量”、“励志”、“感人”的影片以其较强的故事性常常能够引起观众的共鸣，获得大众的欢迎并赢得较高的评价。

### 制片年份分析

在制片年份分析部分
在此选择使用直方图展示数据，优点是可以明显根据图像的面积直观感受数据分布。

```python
ax2 = plt.subplot()
ax2.set_title('各年份上榜次数', fontproperties='Droid Sans Fallback')  # 标题，单处修改字体
ax2.set_xlabel('年份', fontproperties='Droid Sans Fallback')  # x轴标签
ax2.set_xlim(1930, 2020)  # 横轴上下限
ax2.set_ylabel('上榜次数', fontproperties='Droid Sans Fallback')  # y轴标签
ax2.bar(keys_date, values_date)  # 画柱状图
plt.savefig('date_demo.png', dpi=300)
plt.show()
```

得到的图像如下：

![date](/assets/images/date.png)

我们来思考一下，每年上榜的电影数都是可数集，样本空间为
<img src="http://latex.codecogs.com/gif.latex?\bg_white&space;\Omega=1,2,3..." title="\Omega=1,2,3..." />。
取一个极大自然数`n`,我们可以把一个年份分解为等长的`n`段，即<img src="http://latex.codecogs.com/gif.latex?\bg_white&space;(0,\frac{1}{n}],(\frac{1}{n},\frac{2}{n}]...(\frac{n-1}{n},1]" title="(0,\frac{1}{n}],(\frac{1}{n},\frac{2}{n}]...(\frac{n-1}{n},1]" />  
当`n`很大时，每个小区间段有两部电影上榜的概率可以忽略不计。假设每段时间电影上榜的概率相等，即每部电影上榜的概率是独立的，且反比于`n`，不妨假设为 ![](/assets/images/latex003.gif),那么一年内总的上榜电影数为：

![](/assets/latex004.gif)
当![](/assets/latex005.gif) 取极限时,有:
![](/assets/latex006.gif)

我们来总结一下该问题，其满足以下三个特点：

1. 从宏观角度上来看电影上榜TOP250是小概率事件；
2. 电影之间是独立的，也即不会互相依赖或者影响；
3. 电影上榜的概率是稳定的，即必然有250部电影会上榜。

如果某类事件满足上述三个条件，理论上服从泊松分布，所以我们考虑以泊松分布拟合该图像，泊松分布的概率函数为：
![](/assets/latex007.gif)
在这函数中有个重要参数：![](/assets/latex008.gif)， 它代表单位时间内随机事件的平均发生率。

~~拟合图像暂略~~

### 评分与排名关系分析

一般榜单都是按照一个参数（大多为项目评分）降序排列的，评分最高的  
用折线图可以很明显地体现评分与排名的关系趋势

```python
ax3 = plt.subplot()
ax3.set_title('电影排名与评分的关系', fontproperties='Droid Sans Fallback')
ax3.set_xlabel('排名', fontproperties='Droid Sans Fallback')
ax3.set_ylabel('评分', fontproperties='Droid Sans Fallback')
ax3.plot(index_rate, values_rate, 'blue')  # 画折线图
plt.savefig('rate_movie_demo.png', dpi=300)
plt.show()
```

得到折线图如下：  

![](/assets/images/rate_movie.png)

我们通过观察图像可以发现数据并不是单纯的的大致走向是一条曲线，那么很可能符合多项式回归:
![](/assets/images/latex009.gif)
但数据的离散程度还是相当高的，在这我们使用岭回归方法。岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。
在我们尝试以多项式回归拟合该分布时，还有个很重要的参数就是上式中的`n`，它代表多项式的最高指数（`degree`），但是我们不知道指数选择多少比较合适。所以我们只能一个个去试，直到找到最拟合分布的指数。需要注意的是，如果指数选择过大的话可能会导致函数过于拟合， 意味着对数据或者函数未来的发展很难预测，也许指向不同的方向。
如何找到最拟合分布的指数？我们就要考量其误差情况是否合理，常用的误差分析参数主要有确定性系数（`R2`）。

+ `R2`方法是将预测值跟只使用均值的情况下相比，看能好多少，其区间通常在（0,1）之间。如果`R2=0.8`说明变量y的变异性中有80%是由自变量`x`引起的，`R2`越接近`1`代表方程对数据的解释能力越强；如果`R2=1`,表示所有的观测点全部落在回归直线上，即预测跟真实结果完美匹配的情况；如果`R2=0`,则表示自变量与因变量无线性关系，还不如什么都不预测，直接取均值的情况。其计算方法，不同的文献稍微有不同，本文中函数`R2`的实现是使用`sklearn`中的`clf.score`方法。


```python
x = np.array(index_rate)
y = np.array(values_rate)
degree = [1, 3, 10]
ax3 = plt.subplot()
ax3.set_title('电影排名与评分的关系', fontproperties='Droid Sans Fallback')
ax3.set_xlabel('排名', fontproperties='Droid Sans Fallback')
ax3.set_ylabel('评分', fontproperties='Droid Sans Fallback')
ax3.plot(index_rate, values_rate, 'cyan', linewidth=0.7)  # 画折线图
for d in degree:
    clf = Pipeline([('poly', PolynomialFeatures(degree=d)),  # 产生多项式
                    ('linear', linear_model.Ridge())])  # 岭回归
    clf.fit(x[:, np.newaxis], y)
    y_test = clf.predict(x[:, np.newaxis])
    theta_list = clf.named_steps['linear'].coef_
    intercept = clf.named_steps['linear'].intercept_
    r2 = clf.score(x[:, np.newaxis], y)
    print('Poly Degree:' + str(d), 'Theta List:' + str(theta_list), 'Intercept:' + str(intercept),
          'R^2:' + str(r2), '', sep='\n')
    plt.plot(x, y_test, linewidth=2)
plt.grid()
plt.legend(['Data', 'Poly Degree:' + str(degree[0]), 'Poly Degree:' + str(degree[1]), 'Poly Degree:' + str(degree[2])],
           loc='upper right')
plt.savefig('rate_movie_regress_6.png', dpi=300)
plt.show()
```

根据该图做多项式回归,得到的图像如下：

![](/assets/images/rate_movie_regress.png)

其中系数列表`Theta List`、截距`Intercept`和确定性系数`R^2`如下：

```
Poly Degree:1
Theta List:[ 0.         -0.00273477]
Intercept:9.09641419424
R^2:0.493758840232

Poly Degree:3
Theta List:[  0.00000000e+00  -1.17821416e-02   6.88703054e-05  -1.45501357e-07]
Intercept:9.3613611637
R^2:0.57176363556

Poly Degree:10
Theta List:[  0.00000000e+00  -4.08464697e-02   7.41872139e-04   2.25507660e-05
  -1.30316451e-06   2.58355890e-08  -2.75781527e-10   1.73415210e-12
  -6.42779356e-15   1.30013769e-17  -1.10730959e-20]
Intercept:9.56494896273
R^2:0.58663191704
```

可以看到多项式次数为`1`的时候，虽然拟合的不太好，`R2`也能达到`0.494`。`3`次多项式提高到了`0.572`。而指数提高到`10`次，`R2`也只提高到了`0.587`，收效不显著。

一般做回归的时候要求拟合优度越高越好，可以通过增加指数来实现，可是指数变高后增加很多维度那么模型的自由度就减少了，甚至有可能出现过拟合，这些情况的存在往往使得模型复杂度增加而实际意义削弱。例如图中的指数为`10`的曲线模型就存在过度拟合的情况。
最后，按照以上方式多次考察不同指数情况下的曲线，综合得出指数为`6`的多项式曲线表现情况最优，其图像如下：

![](/assets/images/rate_movie_regress_6.png)

此时相应的参数如下：
```
Poly Degree:6
Theta List:[  0.00000000e+00  -2.63742827e-02   5.15743147e-04  -5.79971861e-06
   3.42768652e-08  -9.90538582e-11   1.09647849e-13]
Intercept:9.48161125057
R^2:0.57789270759
```
该曲线可作为数据展示的辅助线，大致描述数值的走向。

### 排名、评分与评论人数综合关系分析

对于大众评论网站来说，评论人数的多少有很大的意义。特别是在中国这样一个人口众多的国家。  
我们综合以上**排名**、**评分**、**评论人数** 这主要的三方面因素，组成一张三维散点图，来展示：

```python
cm = plt.cm.get_cmap('rainbow')
ax4 = plt.subplot()
ax4.set_title('电影评分与评分人数、排名三维关系散点图', fontproperties='Droid Sans Fallback')
ax4.set_xlabel('排名', fontproperties='Droid Sans Fallback')
ax4.set_ylabel('评论人数', fontproperties='Droid Sans Fallback')
sc = ax4.scatter(index_rate, values_comments, c=values_rate, s=8, marker='o', cmap=cm)
plt.colorbar(sc)
plt.savefig('comment_rate_demo.png', dpi=300)
plt.show()
```

得到以下图像：

![comment&amp;rate](/assets/images/comment_rate.png)


当我们仅观察评论人数时，我们可以发现一部影片要做到评论又多而排名又高是件很难的事情，大部分的影片都聚集在坐标轴偏`x`轴的地方。  
但是当我们结合评分高低即散点的颜色来分析时，我们可以发现同样颜色的点的分布近似一条反比例曲线，其中反比例系数随着评分的降低而增加。  
因此可以大胆地猜想一下：**当分数相同时，排名越高的影片打分人数越多？**  
那是什么因素导致这样的现象呢？

1. 我们从人数不断增长对于评分影响的方面来考虑，评分的高低随着评论人数的增长而做一定幅度的震荡变化的最终趋向于稳定。这是评分为算术平均数的一种固有现象，在评论早期人数较少的情况下，个体的差异对与总体的影响较大，
2. 再从电影传播的方面来考虑，早期电影的观众大多为主动观影，更多的是这部影片的爱好者，这批人就是电影制作方想主要抓住的受众,使得评分结果有一定的幸存者偏差。当更多的“好像某某电影上映了，今天没什么事做，要不去看看吧”的观众观影后加入打分行列中，
3. 另一方面，我们从观众的角度来分析，
   具体来说上榜的大部分影片评论人数在三十万以下。可以说明最好的影片是既叫好又叫座的，可以奉为传世经典的电影往往会形成良性循环

综合以上几点，从发展的眼光来看，豆瓣电影的评分结果对于大多数的影片是有一定的参考价值的。


# 总结

这篇文章主要做了以下这些内容：  
1. 收集豆瓣电影排名榜单信息并加以处理成便于调用的文档形式。  
2. 分析榜单中影片相关的数据，并运用了多种不同的图表展示这些数据之间的关系关系。    

经过前几段的简单分析，我们可以发现豆瓣`TOP250`榜单对于国内电影主流文化还是比较契合的。

我们都知道世界上没有一份绝对公平的榜单，世界上的好电影绝对不止`250`部，孰优孰劣这种主观的问题本身就没有绝对的答案。现在大量的算法在研究如何做智能算法、个性化推荐算法、预测型算法，但是算法得到的结果总是客观存在的，以客观事物来揣摩人们的主观内心想法始终是一件很难做得面面俱到的事情。
因此我们也不能忽视此类普适的非个性化推荐算法，这份`TOP250`榜单以其广泛的适应性，在缺乏足够多的数据支撑智能算法分析的情况下来说，是一个很好的解决方案。



### 有待改进的地方

+ 对于评分相同的影片并不一定代表影片质量就相当，还需要考虑评分的标准差、不对称性等高阶数据的影响因素。  
+ 数据可视化部分增加更多交互性，比如显示鼠标悬停位置的数据详情显示。  
+ 数据源的动态分析，增加时间维度的分析。追踪排行榜的变化，相应的自动调整展示的分析结果。  
+ 单一数据源局限性较大，增加与其他榜单如`IMDb`的数据比较可以更加凸显其特点。
