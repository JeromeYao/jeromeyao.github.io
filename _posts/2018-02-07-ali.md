---
layout: post
title: 强化学习在阿里的技术演进与业务创新
description:
date: 2018-02-06 20:57:11 +0800
image: assets/images/thumbnail/ali.png
---

## Part1:阿里技术分享
### 什么是强化学习？
强化学习，是最接近于自然界动物学习的本质的一种学习范式。无论是AlphaGo 在围棋大赛中战胜世界冠军，还是Deepmind 的自动学习玩 ATARI 游戏，背后的强大武器都是深度强化学习技术。
从提出至今，强化学习经历了约半个世纪的发展。但是业界始终没有一本书，能够真正系统地、剖析强化学习技术的落地实践案例。这本书将帮助技术人真正理解强化学习的本质，并且更好地掌握这项技术、用于实践。

### 此书有哪些亮点？
本书首次在工业界系统地披露强化学习在实践应用的技术细节，其中更包含了阿里算法工程师对强化学习的深入理解、思考和创新。此书共有12个章节，作者团队跨越了多个阿里核心算法团队，可谓是最强阵容打造的黄金进阶书籍。

此书覆盖了淘宝、阿里小蜜、广告搜索等多个业务场景：比如在搜索场景中的排序策略决策模型，推荐场景下提高用户和商品的配对效率，在智能客服方面消费者与系统互动的系统决策，以及在广告系统中依靠智能调价技术来实现更好的广告价值与效果，都体现了强化学习技术在一系列决策中的重要角色。

### 谁适合阅读这本书？
无论你是算法工程师、强化学习方向的研究人员，或者是希望转型人工智能领域的机器学习爱好者，都能从本书中汲取所需。
作为算法工程师，你将了解强化学习在实际应用中的建模方法、在业务场景下的常见问题，以及对应的解决思路，提高建模和解决业务问题的能力；
作为强化学习方向的研究人员，你将了解到更多实际的强化学习问题，扩宽研究视野；
作为机器学习爱好者，你将了解到阿里巴巴的一线机器学习算法工程师是如何发现问题、定义问题和解决问题的，将激发你的研究兴趣以及提升专业素养，实现更好的转型。

### 如何下载？
长按识别以下二维码，关注“阿里技术”官方公众号，回复 “强化学习”，即可免费在线阅读、下载此书。

*转载自阿里技术公众号*

---
## Part2：观后感
国内越来越多的技术团队拥抱开源理念，与同行分享业务心得是十分值得让人欣慰的。
能够接触到阿里这种级别的数据量的公司真的是少之又少，在粗略看完本书之后可谓是受益匪浅。
这本书重点描述了阿里巴巴在推动强化学习输出产品及商业化的实践过程。
例如在在搜索场景中对用户的浏览购买行为进行 MDP 建模、在推荐场景中使用深度强化学习与自适应
在线学习帮助每⼀个用户迅速发现宝贝、在智能客服中赋予阿里⼩蜜这类的客服机器⼈对应的决策能力、
在广告系统中实现了基于强化学习的智能调价技术，因而根据顾客的当前状态去决定如何操作调价。


强化学习（RL）是关于序列决策的一种工具，它可以用来解决科学研究、工程文理等学科的一系列问题，
它也是围棋程序 AlphaGo 的重要组成部分。在 Richard Sutton 的描述中，交互式学习几乎是所有学习与智能理论的基石，
而强化学习就是这样的一种理想条件下实现交互式学习的方法。

在探讨阿里的强化学习实践书籍前，我们需要明确几个基本概念。首先，监督学习和强化学习之间的主要区别在于收到的反馈是评估性的还是指导性的。
指导性反馈提示如何达到目标，而评估性反馈告诉你达到目标的程度。监督学习一般是基于指导性反馈来解决问题，而强化学习则基于评估性反馈解决问题。
因此在很多情景中，强化学习这种评估性的反馈使其具有格外的优势与强大的性能。

因为存在这些差别，阿里表明基于监督学习方式的信息提供手段，缺少有效的探索能力，系统倾向于给消费者推送曾经发生过行为的信息单元（商品、店铺或问题答案）。
而强化学习作为⼀种有效的基于用户与系统交互过程建模和最大化过程累积收益的学习方法，在⼀些阿里具体的业务场景中进行了很好的实践并得到⼤规模应用。

我们如果把搜索引擎看作智能体（Agent）、把用户看作环境（Environment），
则商品的搜索问题可以被视为典型的顺序决策问题（Sequential Decision making Problem）。

在以上问题的形式化中，Agent 每⼀次策略的选择可以看成⼀次试错（Trial-and-Error），在这种反复不断地试错过程中，
Agent 将逐步学习到最优的排序策略。而这种在与环境交互的过程中进行试错的学习，正是强化学习（Reinforcement Learning，RL）的根本思想。
除了上述所述基于强化学习的实时搜索排序，阿里在很多任务或功能上都采用了强化学习的解决方案。


---
## Part3：笔记
### 第一章 基于强化学习的实时搜索排序策略调控

用户商品搜索是一种连续性行为。  
用户最终选择购买或不够买商品,不是由某一次排序所决定,而是一连串搜索排序的结果。  
+ 搜索引擎 ==> 智能体(Agent)  
+ 用户 ==> 环境(Environment)  
+ 商品的搜索 ==> 顺序决策问题(Sequential Decision making Problem)  

1. 用户每次请求 PV 时，Agent 做出相应的排序决策，将商品展示给用户；
2. 用户根据 Agent 的排序结果，给出点击、翻页等反馈信号；
3. Agent 接收反馈信号，在新的 PV 请求时做出新的排序决策；
4. 这样的过程将⼀直持续下去，直到用户购买商品或者退出搜索。

强化学习起源于巴甫洛夫条件反射实验，最后抽象为马尔科夫决策过程。
与环境交互的过程中进行试错的学习，正向强化激励循环。

#### 问题建模
##### 强化学习简介

马尔可夫决策过程可以由一个四元组 <S, A, R, T> 表示：
1. S ==> 状态空间(State Space),包含了Agent可能感知到的所有环境状态;
2. A ==> 为动作空间(Action Space),包含了Agent在每个状态上可以采取的所有动作;
3. R ==> S×A×S → R为奖赏函数(Reward Function),R(s, a, s′) 表示在
状态s上执行动作a,并转移到状态s′时,Agent从环境获得的奖赏值;
4. T ==> S×A×S → [0, 1] 为环境的状态转移函数(State Transition Function)。

在与环境的交互过程中,Agent的目标是找到一个**最优策略π∗**  ,使得它在任意状态s和任意时间步骤t下,都能够获得最大的长期累积奖赏:

<img src="http://latex.codecogs.com/gif.latex?\bg_white&space;\pi^*=argmax_{\pi}\mathbf{E}_{\pi}\left&space;\{&space;\sum_{k==0}^{\infty}\gamma^k_{r_{t&plus;k}}|s_t=s\right\},\forall&space;s&space;\in&space;S,\forall&space;t&space;\ge0" title="\pi^*=argmax_{\pi}\mathbf{E}_{\pi}\left \{ \sum_{k==0}^{\infty}\gamma^k_{r_{t+k}}|s_t=s\right\},\forall s \in S,\forall t \ge0" />

+ π : S×A→[0, 1]表示Agent的某个策略(即状态到动作的概率分布)；
+ E_π : 策略π下的期望值；
+ γ∈[0, 1)为折扣率(Discount Rate)；
+ k 为未来时间步骤；
+ r_(t+k)表示Agent在时间步骤(t + k)上获得的即时奖赏。

强化学习主要通过寻找**最优状态值函数**(Optimal State Value Function)

<img src="http://latex.codecogs.com/gif.latex?\bg_white&space;V^*(s)=&space;\max\limits_{\pi}\mathbf{E}_{\pi}\left&space;\{&space;\sum_{k==0}^{\infty}\gamma^k_{r_{t&plus;k}}|s_t=s\right\},\forall&space;s&space;\in&space;S,\forall&space;t&space;\ge0" title="V^*(s)= \max\limits_{\pi}\mathbf{E}_{\pi}\left \{ \sum_{k==0}^{\infty}\gamma^k_{r_{t+k}}|s_t=s\right\},\forall s \in S,\forall t \ge0" />

**最优状态动作值函数**(Optimal State-Action Value Function)

<img src="http://latex.codecogs.com/gif.latex?\bg_white&space;Q^*(s,a)=&space;\max\limits_{\pi}\mathbf{E}_{\pi}\left&space;\{&space;\sum_{k==0}^{\infty}\gamma^k_{r_{t&plus;k}}|s_t=s\right\},\forall&space;s&space;\in&space;S,\forall&space;a&space;\in&space;A,\forall&space;t&space;\ge0" title="Q^*(s,a)= \max\limits_{\pi}\mathbf{E}_{\pi}\left \{ \sum_{k==0}^{\infty}\gamma^k_{r_{t+k}}|s_t=s\right\},\forall s \in S,\forall a \in A,\forall t \ge0" />

来学习最优策略π^∗。

这些经典的算法会产生强化学习中的著名的“维度灾难”问题。

**值函数估计**(Value Function Approximation)是解决维度灾难问题的主要手段之一。  
状态值函数或动作值函数进行参数化,将值函数空间转化为参数空间,达到泛化(Generalization) 的目的。

<img src="http://latex.codecogs.com/gif.latex?\bg_white&space;V_{\theta}(s)=f_{\theta}(\phi(s))" title="V_{\theta}(s)=f_{\theta}(\phi(s))" />

+ φ(s)为状态s的特征向量,
+ f为定义在状态特征空间上的某个函数,其具体形式取决于算法本身。

##### 状态定义


<img src="http://latex.codecogs.com/gif.latex?\bg_white&space;s=(price_1,cvr_1,sale_1,...,price_n,cvr_n,sale_n)" title="s=(price_1,cvr_1,sale_1,...,price_n,cvr_n,sale_n)" />


<img src="http://latex.codecogs.com/gif.latex?\bg_white&space;s=(price_1,cvr_1,sale_1,...,price_n,cvr_n,sale_n,power,item,shop)" title="s=(price_1,cvr_1,sale_1,...,price_n,cvr_n,sale_n,power,item,shop)" />


##### 奖赏函数设定
1. 在一个PV中如果仅发生商品点击,则相应的奖赏值为用户点击的商品的数量;
2. 在一个PV中如果发生商品购买,则相应奖赏值为被购买商品的价格;
3. 其他情况下,奖赏值为0。

第一条是为了提高CTR（Click-Through-Rate）点击通过率。
第二条是为了提高GMV(Gross Merchandise Volume) 成交额。







#### 算法设计

##### 策略函数

##### 策略梯度

##### 值函数的学习

#### 奖赏塑形

#### 实验效果

#### DDPG与梯度融合


#### 总结


### 第二章 延迟奖赏在搜索排序场景中的作用分析
