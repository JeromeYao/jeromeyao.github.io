---
layout: post
title: 机器学习概念小结
description:
date: 2017-11-20 22:13:41 +0800
image: assets/images/thumbnail/ml.png
---


## 理论知识点

#### 过拟合&欠拟合
（举几个例子让判断下，顺便问问交叉验证的目的、超参数搜索方法、EarlyStopping）
三种误差计算方式

**学习曲线**
学习曲线就是比较 jtrain （训练误差）和 jcv（验证误差）。
一般的学习曲线，蓝色的线表示训练集上的误差 jtrain, 粉色的线表示验证集上的误差 jcv，横轴表示训练集合的大小。
**交叉验证**
这里首先解释一下bias和variance的概念。
模型的Error = Bias + Variance，Error反映的是整个模型的准确度，
Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，
Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。
我们可以根据jcv 与 jtrain两个来判断是处于欠拟合还是过拟合。


#### L1正则和L2正则的做法
正则化背后的思想（顺便问问BatchNorm、Covariance Shift）
L1正则产生稀疏解原理

#### 逻辑回归为何线性模型
（顺便问问LR如何解决低维不可分、从图模型角度看LR和朴素贝叶斯和无监督）

#### 几种参数估计方法MLE/MAP/贝叶斯的联系和区别

简单说下SVM的支持向量（顺便问问KKT条件、为何对偶、核的通俗理解）、
GBDT随机森林能否并行（顺便问问bagging boosting）、
生成模型判别模型举个例子、
聚类方法的掌握（顺便问问Kmeans的EM推导思路、谱聚类和Graph-cut的理解）、
梯度下降类方法和牛顿类方法的区别（顺便问问Adam、L-BFGS的思路）、
半监督的思想（顺便问问一些特定半监督算法是如何利用无标签数据的、
从MAP角度看半监督）、
常见的分类模型的评价指标（顺便问问交叉熵、
ROC如何绘制、
AUC的物理含义、
类别不均衡样本）

CNN中卷积操作和卷积核作用、
maxpooling作用、
卷积层与全连接层的联系、
梯度爆炸和消失的概念（顺便问问神经网络权值初始化的方法、为何能减缓梯度爆炸消失、CNN中有哪些解决办法、LSTM如何解决的、如何梯度裁剪、dropout如何用在RNN系列网络中、dropout防止过拟合）、为何卷积可以用在图像/语音/语句上（顺便问问channel在不同类型数据源中的含义）

如果面试者跟我一样做NLP推荐系统，我会继续追问
CRF跟逻辑回归
最大熵模型的关系、
CRF的优化方法、
CRF和MRF的联系、
HMM和CRF的关系（顺便问问 朴素贝叶斯和HMM的联系、LSTM+CRF 用于序列标注的原理、CRF的点函数和边函数、CRF的经验分布）、
WordEmbedding的几种常用方法和原理（顺便问问language model、perplexity评价指标、
word2vec跟Glove的异同）、
topic model说一说、
为何CNN能用在文本分类、
syntactic和semantic问题举例、
常见Sentence embedding方法、
注意力机制（顺便问问注意力机制的几种不同情形、为何引入、seq2seq原理）、
序列标注的评价指标、
语义消歧的做法、
常见的跟word有关的特征、
factorization machine、
常见矩阵分解模型、
如何把分类模型用于商品推荐（包括数据集划分、模型验证等）、
序列学习、
wide&deep model（顺便问问为何wide和deep)

【代码能力】
给出节点的矩阵和边的矩阵，求路径和最大的路径（来源于 Viterbi 算法，本质就是个动态规划），至少给个思路和伪代码（顺便聊聊前向传播和反向传播）

给出一数组，数组元素是pair对，表示一个有向无环图的<父节点, 子节点>，用最优的方法，将其变成一个新的有序数组，数组元素是该有向无环图所有节点，数组的有序性体现在：父亲节点在孩子节点前面（来源于 贝叶斯网络实现时的小trick）
