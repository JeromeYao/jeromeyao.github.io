---
layout: post
title: 中文自然语言处理
description: 工作心得
date: 2017-08-25 21:41:22 +0800
image: assets/images/thumbnail/NLTK.png
---

最近接手了项目做一批主数据处理，客户给定待处理对象（主要为医院、药店或经销商）的基本信息
项目的难点很多首先需要解决的问题在于客户给定对象的名称标准化处理。

使用Python处理中文分词有这几个库，jieba分词、NLKT、THULAC:

1. [fxsjy/jieba](https://github.com/fxsjy/jieba/) SLOGEN：做最好的Python中文分词组件，或许从现在来看它还没做到最好，但是已经做到了使用的人最多。结巴分词网上的学习资料和使用案例比较多，上手相对比较轻松，速度也比较快。之前微信好友签名词云的小案例中就使用到了jieba分词来提取签名中的关键字。
结巴的优点：
* 支持三种分词模式  
* 支持繁体分词  
* 支持自定义词典  
* MIT 授权协议  

2. THULAC：一个高效的中文词法分析工具包前两天我在做有关于共享单车的用户反馈分类，使用jieba分词一直太过零散，分类分不好。THULAC： 由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包 。THULAC的接口文档很详细，简单易上手。
THULAC分词的优点：
* 能力强。利用规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。
* 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％。
* 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度达到1.3MB/s，速度比jieba慢。  

3. NLTK：[Natural Language Toolkit](http://www.nltk.org/)可以说是世界范围最主流的Python自然语言处理库。主要是处理英文对象，相比于前两个库对中文支持就没那么好了。
